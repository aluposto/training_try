Below are two complete, ready-to-run Python scripts: encoder.py (step 1) and decoder.py (step 2). They are written to work with the DCVC DMC model you showed earlier. Each script is robust: it pads frames to the modelâ€™s expected multiples, saves per-frame bitstreams plus a small metadata JSON, and falls back safely if the model returns a direct reconstruction instead of a bitstream.

How to use

1. Put encoder.py on the machine that will encode. Run it against a folder of frames (PNG/JPG) or a single image.


2. Transfer the produced .bin and .json files (one pair per frame) to the decoder machine.


3. Put decoder.py on the machine that will decode. Run it pointed at the directory containing the .bin/.json pairs. The decoder will recreate PNG frames.



Both scripts assume the repo with src is importable (i.e. run with PYTHONPATH=$PWD python encoder.py ... if needed), and both will attempt to call model.update() to initialize entropy tables (silently continue if unavailable).


---

encoder.py (encoding / producing .bin + .meta.json per frame)

#!/usr/bin/env python3
"""
encoder.py
Usage example:
  PYTHONPATH=$PWD python encoder.py --frames-dir data/frames/train/clip_01 \
    --out-dir encoded/clip_01 --qp 0

Produces files:
  encoded/clip_01/frame_000000.bin      # entropy-coded bitstream OR empty if none
  encoded/clip_01/frame_000000.meta.json
If model.compress() returns a direct reconstruction x_hat instead of bitstream,
encoder will save 'raw_xhat': true in meta and also write xhat PNG to same folder.
"""
import os, json, argparse, glob, math
from pathlib import Path
from PIL import Image
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm

# import model class
from src.models.video_model import DMC as VideoModel

def pad_to_multiple(x, mult=64):
    # x: torch tensor [1,C,H,W]
    _, _, H, W = x.shape
    pad_h = (mult - (H % mult)) % mult
    pad_w = (mult - (W % mult)) % mult
    if pad_h == 0 and pad_w == 0:
        return x, (0,0)
    pad = (0, pad_w, 0, pad_h)  # left, right, top, bottom
    x_p = F.pad(x, pad, mode='replicate')
    return x_p, (pad_h, pad_w)

def main():
    p = argparse.ArgumentParser()
    p.add_argument('--frames-dir', type=str, required=True, help="Directory with input frames (png/jpg).")
    p.add_argument('--out-dir', type=str, required=True, help="Directory to write .bin + .meta.json outputs.")
    p.add_argument('--qp', type=int, default=0, help="QP index to use (0..63).")
    p.add_argument('--device', type=str, default='cuda', help="cuda or cpu")
    p.add_argument('--pad-mult', type=int, default=64, help="pad H/W to multiple of this before encoding")
    args = p.parse_args()

    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    os.makedirs(args.out_dir, exist_ok=True)

    # instantiate model
    model = VideoModel().to(device).eval()
    try:
        model.update()
    except Exception:
        # ok if not available
        pass

    # clear DPB if present
    try:
        model.clear_dpb()
    except Exception:
        model.dpb = []

    # list frames
    frames = sorted(glob.glob(os.path.join(args.frames_dir, '*.*')))
    frames = [f for f in frames if f.lower().endswith(('.png','.jpg','.jpeg','bmp','tiff'))]
    if len(frames) == 0:
        raise SystemExit("No frames found in frames-dir")

    for i, fp in enumerate(tqdm(frames, desc="Encoding frames")):
        im = Image.open(fp).convert('RGB')
        arr = np.asarray(im, dtype=np.float32) / 255.0
        x = torch.from_numpy(arr.transpose(2,0,1))[None,...].to(device)  # [1,C,H,W]

        # pad
        x_p, (pad_h, pad_w) = pad_to_multiple(x, mult=args.pad_mult)

        # ensure dpb has a frame (some models expect dpb[0])
        if i == 0:
            zeros = torch.zeros_like(x_p)
            try:
                model.clear_dpb()
            except Exception:
                model.dpb = []
            try:
                model.add_ref_frame(frame=zeros, increase_poc=False)
            except TypeError:
                # some signatures differ
                try:
                    model.add_ref_frame(feature=None, frame=zeros, increase_poc=False)
                except Exception:
                    pass

        meta = {
            'orig_path': os.path.basename(fp),
            'height': int(x.shape[2]),
            'width': int(x.shape[3]),
            'pad_h': int(pad_h),
            'pad_w': int(pad_w),
            'qp': int(args.qp),
            'ec_part': 0,
            'model': 'DMC',
        }

        # Call compress (best-effort). compress may raise if compiled coder missing -- handle gracefully.
        try:
            out = model.compress(x_p, int(args.qp))
        except Exception as e:
            # fallback: try a differentiable run-through of encoder/decoder submodules to obtain x_hat
            out = None
            try:
                # do NOT use torch.no_grad() so gradients not required here; but it's okay to run w/ no grad for inference
                with torch.no_grad():
                    feat_in = F.pixel_unshuffle(x_p, 8) if hasattr(F, "pixel_unshuffle") else x_p
                    try:
                        feat = model.feature_adaptor_i(feat_in)
                    except Exception:
                        feat = feat_in
                    ctx, ctx_t = (None, None)
                    if hasattr(model, "feature_extractor"):
                        try:
                            qf = model.q_feature[0:1] if hasattr(model, "q_feature") else None
                            ctx, ctx_t = model.feature_extractor(feat, qf)
                        except Exception:
                            ctx, ctx_t = (None, None)
                    y = None
                    if hasattr(model, "encoder"):
                        try:
                            q_enc = model.q_encoder[0:1] if hasattr(model, "q_encoder") else None
                            y = model.encoder(x_p, ctx if ctx is not None else feat, q_enc)
                        except Exception:
                            y = None
                    if y is not None and hasattr(model, "decoder") and hasattr(model, "recon_generation_net"):
                        q_dec = model.q_decoder[0:1] if hasattr(model, "q_decoder") else None
                        q_recon = model.q_recon[0:1] if hasattr(model, "q_recon") else None
                        feat_dec = model.decoder(y, ctx if ctx is not None else feat, q_dec)
                        x_hat = model.recon_generation_net(feat_dec, q_recon).clamp(0,1)
                        out = {'bit_stream': None, 'x_hat': x_hat}  # mark as raw
            except Exception:
                out = None

        # write outputs
        idx = i
        bin_path = os.path.join(args.out_dir, f"frame_{idx:06d}.bin")
        meta_path = os.path.join(args.out_dir, f"frame_{idx:06d}.meta.json")

        if out is None:
            # no result -> treat as raw identity (save raw_xhat)
            x_hat = x_p
            meta['raw_xhat'] = True
            meta['note'] = 'compress_failed_fallback_identity'
            # save x_hat PNG
            out_png = os.path.join(args.out_dir, f"frame_{idx:06d}.raw_xhat.png")
            img_np = (x_hat[0].cpu().numpy().transpose(1,2,0) * 255.0).round().astype('uint8')
            Image.fromarray(img_np).save(out_png)
            # create empty .bin file for consistency
            open(bin_path, 'wb').close()
        else:
            bs = out.get('bit_stream', None)
            xhat_from_out = out.get('x_hat', None)
            if bs is None:
                # model didn't produce bitstream (image model/fallback) -> store x_hat as PNG and mark meta
                meta['raw_xhat'] = True
                if xhat_from_out is None:
                    # if nothing, fallback identity
                    x_hat = x_p
                else:
                    x_hat = xhat_from_out
                out_png = os.path.join(args.out_dir, f"frame_{idx:06d}.raw_xhat.png")
                img_np = (x_hat[0].cpu().numpy().transpose(1,2,0) * 255.0).round().astype('uint8')
                Image.fromarray(img_np).save(out_png)
                open(bin_path, 'wb').close()
            else:
                # we have a real bitstream
                meta['raw_xhat'] = False
                # write bytes
                with open(bin_path, 'wb') as f:
                    if isinstance(bs, (bytes, bytearray)):
                        f.write(bs)
                    else:
                        # if coder returned a numpy buffer or other, attempt bytes()
                        try:
                            f.write(bytes(bs))
                        except Exception:
                            # last resort: if it's a list of bytes chunks
                            if isinstance(bs, (list,tuple)):
                                for chunk in bs:
                                    f.write(bytes(chunk))
                            else:
                                raise

        # write metadata JSON
        with open(meta_path, 'w') as f:
            json.dump(meta, f, indent=2)

    print("Encoding finished. Wrote", len(frames), "frames to", args.out_dir)

if __name__ == "__main__":
    main()


---

decoder.py (decoding / reading .bin + .meta.json and reconstructing PNGs)

#!/usr/bin/env python3
"""
decoder.py
Usage example:
  PYTHONPATH=$PWD python decoder.py --encoded-dir encoded/clip_01 --out-dir recon/clip_01_decoded --device cuda

This reads frame_XXXXXX.bin + frame_XXXXXX.meta.json pairs and writes PNG reconstructions.
"""
import os, json, argparse, glob, math
from pathlib import Path
from PIL import Image
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm

# import model class
from src.models.video_model import DMC as VideoModel

def crop_from_padded(x_padded, orig_h, orig_w):
    # x_padded: [1,C,H_p,W_p]
    return x_padded[..., :orig_h, :orig_w]

def main():
    p = argparse.ArgumentParser()
    p.add_argument('--encoded-dir', type=str, required=True, help="Directory containing .bin and .meta.json pairs.")
    p.add_argument('--out-dir', type=str, required=True, help="Directory to write reconstructed frames (PNGs).")
    p.add_argument('--device', type=str, default='cuda', help="cuda or cpu")
    p.add_argument('--pad-mult', type=int, default=64, help="padding multiple used during encoding (must match).")
    args = p.parse_args()

    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    os.makedirs(args.out_dir, exist_ok=True)

    # instantiate model
    model = VideoModel().to(device).eval()
    try:
        model.update()
    except Exception:
        print("model.update() failed or not needed (continuing fallback).")

    # clear DPB
    try:
        model.clear_dpb()
    except Exception:
        model.dpb = []

    # find meta files
    metas = sorted(glob.glob(os.path.join(args.encoded_dir, "frame_*.meta.json")))
    if len(metas) == 0:
        raise SystemExit("No meta files found in encoded-dir")

    bad_frames = 0
    last_good_frame = None

    for meta_file in tqdm(metas, desc="Decoding frames"):
        with open(meta_file, 'r') as f:
            meta = json.load(f)
        base = os.path.basename(meta_file).replace('.meta.json','')
        bin_path = os.path.join(args.encoded_dir, f"{base}.bin")
        idx_str = base.split('_')[-1]
        out_png = os.path.join(args.out_dir, f"frame_{idx_str}.png")
        orig_h = int(meta.get('height'))
        orig_w = int(meta.get('width'))
        pad_h = int(meta.get('pad_h', 0))
        pad_w = int(meta.get('pad_w', 0))
        qp = int(meta.get('qp', 0))
        raw_xhat = bool(meta.get('raw_xhat', False))

        # If raw_xhat -> load PNG generated by encoder or fallback identity
        if raw_xhat:
            raw_png = os.path.join(args.encoded_dir, f"{base}.raw_xhat.png")
            if os.path.exists(raw_png):
                img = Image.open(raw_png).convert('RGB')
                img.save(out_png)
                last_good_frame = out_png
                continue
            else:
                # no raw file; attempt fallback: identity (not ideal)
                print(f"Warning: raw_xhat true but {raw_png} missing. Using black frame.")
                black = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
                Image.fromarray(black).save(out_png)
                last_good_frame = out_png
                continue

        # read bitstream bytes
        if not os.path.exists(bin_path):
            print(f"Warning: bin file missing for {meta_file}, creating black frame")
            black = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
            Image.fromarray(black).save(out_png)
            bad_frames += 1
            continue

        with open(bin_path, 'rb') as f:
            bs = f.read()

        # Try decompress via model.decompress
        try:
            sps = {'height': orig_h + pad_h, 'width': orig_w + pad_w, 'ec_part': meta.get('ec_part', 0)}
            dec = model.decompress(bs, sps, qp)
            x_hat = dec.get('x_hat', None)
            if x_hat is None:
                raise RuntimeError("decompress returned no x_hat")
            # ensure device and detach
            if isinstance(x_hat, torch.Tensor):
                x_hat = x_hat.detach().cpu()
            else:
                # if numpy array
                x_hat = torch.from_numpy(np.array(x_hat)).float()

            # crop to original size
            x_hat = crop_from_padded(x_hat, orig_h, orig_w)
            out_np = (np.clip(x_hat[0].numpy().transpose(1,2,0), 0.0, 1.0) * 255.0).round().astype(np.uint8)
            Image.fromarray(out_np).save(out_png)
            last_good_frame = out_png

            # update DPB if model expects it (best-effort)
            try:
                # attempt to add frame to dpb similar to encoder
                t = torch.from_numpy(out_np.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).to(device)
                t_padded = t
                # add as reference frame
                try:
                    model.add_ref_frame(feature=None, frame=t_padded, increase_poc=True)
                except Exception:
                    try:
                        model.add_ref_frame(frame=t_padded, increase_poc=True)
                    except Exception:
                        pass
            except Exception:
                pass

        except Exception as e:
            # decompression failed: save last_good_frame (if available) or black frame
            bad_frames += 1
            print(f"Warning: decompress failed for {meta_file}: {e}")
            if last_good_frame is not None:
                # copy last good to current
                from shutil import copyfile
                copyfile(last_good_frame, out_png)
            else:
                black = np.zeros((orig_h, orig_w, 3), dtype=np.uint8)
                Image.fromarray(black).save(out_png)

    print("Decoding finished. Reconstructed frames saved to", args.out_dir)
    print("Bad frames replaced:", bad_frames)

if __name__ == "__main__":
    main()


---

Important notes & tips

Pad multiple: I used 64 by default to match DCVC internals; change --pad-mult if you used a different padding strategy when training/encoding.

Model/extension compatibility: The encoder uses model.compress() (which relies on the entropy coder library). The decoder uses model.decompress() â€” both machines must have matching model code and the MLCodec_extensions_cpp entropy coder compiled/installed to decode bitstreams. If the decoder cannot run the entropy coder extension, use the fallback approach (save raw quantized tensors or raw x_hat PNGs from the encoder) â€” encoder already handles missing compress() by producing a raw_xhat.

Metadata: The .meta.json stores original height/width and padding so decoder can correctly crop.

DPB: the scripts call model.add_ref_frame/model.clear_dpb best-effort so temporal priors behave similarly on encoder & decoder. If your encoder uses a different DPB strategy (GOP/I-frame insertion), ensure decoder receives the same DPB resets / sequence boundaries.

Testing locally: Before distributing, test decompress(compress(x)) locally to confirm bit-exact or close reconstructions.

Transferring files: For many frames you may prefer bundling e.g. tar/zip or using object storage (S3) rather than many individual files.



---

If you want, I can:

Add an option to the encoder to save quantized latent tensors instead of bitstreams (for usage on decoders without entropy coder).

Provide an example that writes a single container file (one big .tar or .npz) instead of per-frame files.

Show how to export the decoder neural nets to TorchScript/ONNX so the decoder can run without the original Python repo (still requires entropy decoder or you must send latents).


Which follow-up would you like?
