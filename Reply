Thanks — you’ve given a great debug signal: retraining to 30 epochs didn’t fix it and you still see a few frames that produce NaN (so the problem is not just the single long run). Let’s stop the guessing and fix this methodically.

Below I give a short, prioritized plan (fast → deeper), plus copy-paste scripts you can run right now to (A) find which frames are bad, (B) find which layer produces the first NaN, (C) guard/sanitize outputs so your evaluation doesn’t blow up, and (D) training changes to prevent recurrence. Follow steps in order until the problem disappears.


---

Quick summary of likely root causes

Numerical instability during inference from model weights (NaN in parameters) or activations produced by some operation (custom op, division by small numbers, unstable residual, AMP overflow).

Intermittent, input-dependent failure—only a few frames produce NaNs. That implies certain spatial sizes, contents, or padding edge-cases trigger the issue.

Custom/C++ ops (entropy coder, cuda_inference) are possible culprits.



---

RUN THIS FIRST — find the bad frames (fast)

This tells you exactly which frames produce non-finite pixels after reconstruction.

Save as find_bad_frames.py and run (PYTHONPATH=$PWD python find_bad_frames.py) or copy into a notebook cell.

import os, glob, numpy as np, torch
from PIL import Image
from src.models.video_model import DMC as VideoModel
from training import encode_decode_sequence   # your wrapper
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# load model (use checkpoint that you used for inference)
ckpt = "./checkpoints_finetune_lambda0.01/best_epoch001.pth.tar"
model = VideoModel().to(device).eval()
if os.path.exists(ckpt):
    ck = torch.load(ckpt, map_location='cpu')
    sd = ck.get('state_dict', ck)
    model.load_state_dict({k.replace("module.",""):v for k,v in sd.items()}, strict=False)
    print("Loaded checkpoint:", ckpt)

frame_paths = sorted(glob.glob("data/frames/val/clip_01/*.png"))
if len(frame_paths) == 0:
    frame_paths = sorted(glob.glob("data/frames/train/clip_01/*.png"))

bad = []
for i,p in enumerate(frame_paths):
    im = np.asarray(Image.open(p).convert("RGB"), dtype=np.float32)/255.0
    x = torch.from_numpy(im.transpose(2,0,1))[None].to(device)   # [1,3,H,W]
    out = encode_decode_sequence(model, x.unsqueeze(1), qp=0)   # [B,T,C,H,W]
    x_hat = out['x_hat'][:,0]                                    # [B,C,H,W]
    a = x_hat[0].cpu().numpy()
    if not np.isfinite(a).all():
        print("BAD frame:", i, p, "min/max:", np.nanmin(a), np.nanmax(a))
        bad.append((i,p))
        # save debug dump
        np.save(f"debug_nan_frame_{i}.npy", a)
print("Done. Found", len(bad), "bad frames.")

Result: you’ll get a list of specific filenames (e.g. frame_00447.png) that produce NaNs. Stop here and run the next step only if there are bad frames.


---

If bad frames exist — find the first module that generates NaN

This will pinpoint the layer/operator causing the NaN. Run this for one bad frame (change frame_paths[INDEX] to a bad frame found above).

import torch, numpy as np, glob, os
from PIL import Image
from src.models.video_model import DMC as VideoModel
from training import encode_decode_sequence
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = VideoModel().to(device).eval()
ckpt = "./checkpoints_finetune_lambda0.01/best_epoch001.pth.tar"
ck = torch.load(ckpt, map_location='cpu')
sd = ck.get('state_dict', ck)
model.load_state_dict({k.replace("module.",""):v for k,v in sd.items()}, strict=False)
print("Loaded", ckpt)

# pick one known bad frame from previous run
frame_path = "data/frames/val/clip_01/000447.png"   # REPLACE with actual bad file
img = np.asarray(Image.open(frame_path).convert("RGB"), dtype=np.float32)/255.0
x = torch.from_numpy(img.transpose(2,0,1))[None].to(device)  # [1,3,H,W]

# ensure dpb / ref frame exists and matches padded size
try:
    zeros = torch.zeros_like(x)
    try:
        model.clear_dpb()
    except Exception:
        model.dpb = []
    try:
        model.add_ref_frame(frame=zeros, increase_poc=False)
    except Exception:
        pass
except Exception:
    pass

found = False
def hook_fn(module, inp, out):
    global found
    if found:
        return
    try:
        if isinstance(out, torch.Tensor):
            if not torch.isfinite(out).all():
                print("NaN/Inf in module:", module.__class__, "min/max:",
                      float(torch.nanmin(out)), float(torch.nanmax(out)))
                found = True
    except Exception as e:
        print("Hook error:", e)

handles = [m.register_forward_hook(hook_fn) for m in model.modules()]

with torch.no_grad():
    # run inference on this single frame via wrapper; wrapper will call compress/decompress or fallback
    _ = encode_decode_sequence(model, x.unsqueeze(1), qp=0)

for h in handles: h.remove()
if not found:
    print("No module produced NaN in this forward trace (maybe produced earlier/later).")

What to do with output:

If a module name appears (for example DepthConvBlock or ResidualBlockWithStride2 or something from cuda_inference), paste that module name + printed min/max here and I’ll give targeted code fixes (e.g., add clamp, eps in divisions, or replacing problematic ops).

If the first NaN appears in a custom CUDA extension (files under src/layers/cuda_inference or MLCodec_extensions_cpp), we'd need to check that extension or fallback to the pure-PyTorch code path.



---

Short-term safety / inference sanitization (so you can evaluate safely)

If you only want to avoid crashes and get usable outputs while debugging:

1. Sanitize outputs before saving (replace NaN/Inf and clip):



x_hat_np = x_hat[0].cpu().numpy().transpose(1,2,0)  # H,W,3
x_hat_np = np.nan_to_num(x_hat_np, nan=0.0, posinf=1.0, neginf=0.0)
x_hat_np = np.clip(x_hat_np, 0.0, 1.0)
out_np = (x_hat_np * 255.0).round().astype(np.uint8)

2. Compute metrics robustly (ignore NaNs):



mse = np.nanmean((orig_np - x_hat_np)**2)
psnr = 10*np.log10(1.0 / mse) if np.isfinite(mse) and mse > 0 else float('nan')

This prevents the invalid value encountered in cast warning and gives an avg_psnr computed over finite frames. But this is only a stopgap — you should still find & fix the source.


---

Training-side defenses (prevent NaNs from appearing)

Make these training changes — they are standard fixes for numerical instability:

1. Disable AMP (remove --amp) — run in FP32 to see if problem persists. FP16 overflows are common culprits.


2. Lower learning rate — try --lr 5e-5 or 1e-5 (your default was 2e-4).


3. Stronger gradient clipping — set --max-norm 0.5 (you used 1.0 before).


4. Increase weight decay slightly: --weight-decay 1e-5 or 1e-4.


5. Save checkpoints every epoch and resume from last good checkpoint if NaNs appear. (Patch training to save each epoch.)


6. Monitor grad norms each step and abort if > threshold (e.g. 1e3). Add snippet after backward to log grad_norm. I gave the snippet earlier; add it to run_epoch.



Example safer resume command:

PYTHONPATH=$PWD python training.py \
  --train-glob "data/frames/train/**/*.png" \
  --val-glob "data/frames/val/**/*.png" \
  --patch-size 256 --temporal-len 4 --batch-size 6 \
  --epochs 100 --cuda \
  --pretrained "./checkpoints_finetune_lambda0.01/best_epoch001.pth.tar" \
  --lr 5e-5 --weight-decay 1e-5 \
  --save-dir "./checkpoints_safe" \
  --log-dir "./logs_safe" \
  --workers 4 --max-norm 0.5


---

If it’s a custom/native op bug

Your logs earlier had this message: cannot import cuda implementation for inference, fallback to pytorch. If the native cuda implementation is absent, the code ran in pure-PyTorch; that generally is more stable but slower. If you do sometimes import the CUDA implementation, it may behave differently — inconsistent runtime paths can cause differing numerical behavior. Make sure you consistently use either cuda PyTorch path or native path.

If diagnostic hooks point at code in src/layers/cuda_inference or MLCodec_extensions_cpp, you may need to rebuild or fallback to pure-PyTorch implementation. I can give exact build commands if you want.



---

Suggested immediate steps for you (in this order)

1. Run Find bad frames script above. If zero bad frames → tell me and we’ll inspect other causes. If there are bad frames, continue:


2. Run the hook script for one bad frame — paste its output here. I’ll give a targeted fix for the failing module.


3. Meanwhile, patch your inference script to sanitize outputs so evaluation runs without crashing and you can inspect the saved debug_nan_frame_*.npy files. Use the nan_to_num/clamp snippet above.


4. If you prefer a fast pragmatic fix: retrain with disabled AMP and lower LR / smaller grad norm as shown. That often removes the instability immediately.




---

Extra: Useful one-shot checks (paste their outputs here)

Run these 3 quick checks and paste the text back — they help me identify if parameters are corrupted:

A. Any NaNs in model params?

import torch
from src.models.video_model import DMC as VideoModel
m = VideoModel().cuda().eval()
ck = torch.load("./checkpoints_finetune_lambda0.01/best_epoch001.pth.tar", map_location='cpu')
sd = ck.get('state_dict', ck)
m.load_state_dict({k.replace('module.',''):v for k,v in sd.items()}, strict=False)
any_nan = False
for n,p in m.named_parameters():
    if not torch.isfinite(p).all():
        print("NaN in param:", n)
        any_nan = True
print("any param NaN:", any_nan)

B. Small forward/test run statistics (one frame):

x = ... # load one frame as tensor on cuda
with torch.no_grad():
    out = encode_decode_sequence(m, x.unsqueeze(1), qp=0)
    xhat = out['x_hat'][:,0]
    print("xhat min/max finite:", float(torch.nanmin(torch.where(torch.isfinite(xhat), xhat, torch.tensor(0.0)))), float(torch.nanmax(torch.where(torch.isfinite(xhat), xhat, torch.tensor(0.0)))))

C. Print modules list (useful when you paste the failing module name)

for m in m.modules():
    print(type(m))


---

Final notes / expected timeline

Most often these NaNs are fixed by disabling AMP and reducing LR (fastest fix). If that doesn’t help, the hook method pinpoints the guilty op and we can patch it (usually adding small eps or clamping intermediate tensors).

If hook points to a compiled extension, I’ll give rebuild instructions or a fallback suggestion.



---

Tell me which of the three you want me to run for you next:

1. I’ll provide a ready-to-run find_bad_frames.py and hook_debug.py (I already gave them — run them and paste outputs).


2. I’ll patch training.py to add grad-norm logging, epoch checkpointing, and early rollback (I can produce the patch).


3. I’ll give a single-line command to safely resume training (disable AMP + lower LR + stronger clip) and explain how to monitor.



If you run the find-bad-frames script and paste one bad frame filename here I’ll run the module-hook analysis and give a specific code fix.

