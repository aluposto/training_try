# padded_reconstruct_clip.py (paste in notebook cell)
import os, math, glob, tqdm
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F
from src.models.video_model import DMC as VideoModel

# Paths - edit if needed
ckpt_path = "./checkpoints_finetune_lambda0.01/best_epoch010.pth.tar"
frames_dir = "data/frames/train/clip_01"   # update as needed
out_dir = "outputs/recon_frames/clip_01_padded"
os.makedirs(out_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("device:", device)

# Load model
model = VideoModel().to(device).eval()
if os.path.exists(ckpt_path):
    ck = torch.load(ckpt_path, map_location="cpu")
    sd = ck.get("state_dict", ck)
    model.load_state_dict({k.replace("module.",""): v for k,v in sd.items()}, strict=False)
    print("Loaded checkpoint:", ckpt_path)
else:
    print("Warning: checkpoint not found:", ckpt_path)

# Helper: PSNR
def mse_to_psnr(mse, max_val=1.0):
    if mse <= 0: 
        return float("inf")
    return 10.0 * math.log10((max_val**2) / mse)

# helper to compute padding so H and W become multiples of 16
def get_pad_amount(height, width, mult=16):
    pad_h = (mult - (height % mult)) % mult
    pad_w = (mult - (width % mult)) % mult
    # pad format for torch.nn.functional.pad: (left, right, top, bottom)
    left = 0
    right = pad_w
    top = 0
    bottom = pad_h
    return (left, right, top, bottom)

# get frame list
frame_paths = sorted(glob.glob(os.path.join(frames_dir, "*.png")))
if len(frame_paths) == 0:
    raise SystemExit(f"No frames found in {frames_dir}")

# initialize dpb
try:
    model.clear_dpb()
except Exception:
    model.dpb = []
# We'll initialize dpb with a zero frame of the padded size for the first frame; created inside loop

psnr_list = []
mse_list = []

for i, p in enumerate(tqdm.tqdm(frame_paths, desc="Frames")):
    im = Image.open(p).convert("RGB")
    arr = np.asarray(im, dtype=np.float32)/255.0
    H, W = arr.shape[0], arr.shape[1]
    # compute pad to multiple of 16
    left, right, top, bottom = get_pad_amount(H, W, mult=16)  # WATCH: we will pad height/width accordingly but F.pad expects (left,right,top,bottom)
    # Note: arr shape is (H,W,3). We'll convert to tensor [1,C,H,W] then pad.
    x = torch.from_numpy(arr.transpose(2,0,1))[None,...].to(device)  # [1,C,H,W]

    # We computed pad amounts assuming (height, width) -> but F.pad takes (left,right,top,bottom) for last two dims.
    # So compute right/bottom correctly:
    pad_h = (16 - (H % 16)) % 16
    pad_w = (16 - (W % 16)) % 16
    pad = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)
    if pad_w != 0 or pad_h != 0:
        x_padded = F.pad(x, pad, mode='replicate')  # replicate pad is safe
    else:
        x_padded = x

    # For first frame ensure dpb has a zero frame with same padded dims
    if i == 0:
        zeros = torch.zeros_like(x_padded)
        try:
            model.clear_dpb()
        except Exception:
            model.dpb = []
        try:
            model.add_ref_frame(frame=zeros, increase_poc=False)
        except Exception:
            # some variants expect different signature
            try:
                model.add_ref_frame(feature=None, frame=zeros, increase_poc=False)
            except Exception:
                # fallback: set dpb directly
                model.dpb = []
                model.add_ref_frame(frame=zeros, increase_poc=False)

    # run best-effort differentiable encode-decode path (no entropy coder)
    with torch.no_grad():
        # apply feature adaptor (uses dpb[0])
        try:
            feat_adapt = model.apply_feature_adaptor()
        except Exception:
            # fallback: run feature_adaptor_i on pixel_unshuffle of padded input
            try:
                feat_in = F.pixel_unshuffle(x_padded, 8)
            except Exception:
                feat_in = x_padded
            try:
                feat_adapt = model.feature_adaptor_i(feat_in)
            except Exception:
                feat_adapt = feat_in

        # feature_extractor
        ctx = None; ctx_t = None
        if hasattr(model, "feature_extractor"):
            try:
                qf = model.q_feature[0:1] if hasattr(model,"q_feature") else None
                ctx, ctx_t = model.feature_extractor(feat_adapt, qf)
            except Exception:
                ctx, ctx_t = (None, None)

        # encoder
        y = None
        if hasattr(model, "encoder"):
            try:
                q_enc = model.q_encoder[0:1] if hasattr(model,"q_encoder") else None
                y = model.encoder(x_padded, ctx if ctx is not None else feat_adapt, q_enc)
            except Exception:
                y = None

        # decoder + recon net
        x_hat_padded = x_padded
        if y is not None and hasattr(model, "decoder") and hasattr(model, "recon_generation_net"):
            try:
                q_dec = model.q_decoder[0:1] if hasattr(model,"q_decoder") else None
                q_recon = model.q_recon[0:1] if hasattr(model,"q_recon") else None
                feat_dec = model.decoder(y, ctx if ctx is not None else feat_adapt, q_dec)
                x_hat_padded = model.recon_generation_net(feat_dec, q_recon).clamp(0,1)
                # update dpb for next frame
                try:
                    model.add_ref_frame(feature=feat_dec, frame=x_hat_padded, increase_poc=True)
                except Exception:
                    try:
                        model.add_ref_frame(frame=x_hat_padded, increase_poc=True)
                    except Exception:
                        pass
            except Exception as e:
                # fallback identity if decoder fails
                # print("decoder failed:", e)
                x_hat_padded = x_padded

    # crop back to original H,W
    x_hat = x_hat_padded[..., :H, :W]  # crop bottom/right if padded

    mse = F.mse_loss(x_hat, x, reduction="mean").item()
    psnr = mse_to_psnr(mse)
    mse_list.append(mse)
    psnr_list.append(psnr)

    # save reconstructed frame (clipped)
    out_np = (x_hat[0].cpu().numpy().transpose(1,2,0) * 255.0).round().astype(np.uint8)
    Image.fromarray(out_np).save(os.path.join(out_dir, f"frame_{i:05d}.png"))

    if i < 3:
        print(f"frame {i}: mse={mse:.6e} psnr={psnr if psnr!=float('inf') else 'inf'} saved -> {os.path.join(out_dir, f'frame_{i:05d}.png')}")

# summary
finite_psnrs = [p for p in psnr_list if np.isfinite(p)]
avg_mse = float(np.mean(mse_list))
avg_psnr = float(np.mean(finite_psnrs)) if len(finite_psnrs)>0 else float('nan')
print("Done. frames:", len(frame_paths), "avg_mse:", avg_mse, "avg_psnr (finite only):", avg_psnr)
print("Saved reconstructed frames to:", out_dir)
